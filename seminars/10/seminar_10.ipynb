{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d50089a-8c01-455a-85ae-efd82fc10941",
   "metadata": {},
   "source": [
    "# Семинар 10: Ядерные методы (Kernel methods) и машина опорных векторов (Support vector machine). Kernelized Nearest Neighbor. Kernelized Ridge Regression. Kernelized LASSO Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b4127-f490-4e5b-836c-4bedee87f9ee",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SergeyMalashenko/MachineLearning_Summer_2023/blob/main/seminars/10/seminar_10.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf1ca65-39ae-4f39-a49b-81acdc8dff62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import scipy             as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa2226-caa5-4e77-a986-0a8df10596cd",
   "metadata": {},
   "source": [
    "## Теорема Ковера (Cover's theorem)\n",
    "Является утверждением в теория вычислительного обучения и является одним из основных теоретических мотивов использования нелинейных методов ядра в приложениях машинного обучения . Теорема утверждает, что для данного набора обучающих данных, который не является линейно разделимым , можно с высокой вероятностью преобразовать его в обучающий набор, который линейно разделяется, проецируя его в многомерное пространство через некоторые преобразования.\n",
    "\n",
    "![](images/example.png)\n",
    "\n",
    "Как следствие теорема говорит о том, что если взять свой набор данных и преобразовать эти точки в пространство более высокой размерности, то возможно построить модель линейной классификации (классификатор). Однако большинство классификаторов должны вычислять некоторое подобие, например, точечное произведение, а это означает, что временная сложность алгоритма классификации пропорциональна размерности точки данных. Таким образом, большая размерность означает большую временную сложность (не говоря уже о сложности хранения точек большой размерности).\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "Пусть $M$ исходная размерность точек данных, а f - преобразование, отображающее эти точки в пространство размерности $N(>>M)$. Теперь, если существует функция K, которая принимает входные данные x и y из исходного пространства и вычисляет $K(x,y)=⟨f(x),f(y)⟩$, то я могу вычислить точечное произведение в пространстве более высокой размерности, но со сложностью $O(M)$ вместо $O(N)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc69f6-dafd-44ca-8aa1-0641167c4a6e",
   "metadata": {},
   "source": [
    "## Ядерные функции (Kernel function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2a317-8dd4-4891-944d-bfde34f8090b",
   "metadata": {},
   "source": [
    "### Ядро RBF (Radial Basis Function)\n",
    "\n",
    "$$k(x, x') = exp(-\\frac{1}{2}(x-x')\\Sigma^{-1}(x-x'))$$\n",
    "\n",
    ", если матрица $\\Sigma$ диагональная\n",
    "\n",
    "$$k(x, x') = exp(-\\frac{1}{2}\\Sigma_{i=1}^{D}\\frac{1}{\\sigma_i^2}(x-x')^2)$$\n",
    "\n",
    ", если $\\Sigma$ диагональная и изотропная\n",
    "\n",
    "$$k(x, x') = exp(-\\frac{1}{2{\\sigma}^2} \\|x - x' \\|^2   )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b50260-37ee-4199-8ee4-cb6da4a2ac36",
   "metadata": {},
   "source": [
    "### Ядра Мерсера (Mercer kernel)\n",
    "\n",
    "Преобразование можно представить следующей матрицей Грамма\n",
    "\n",
    "$$K(x, x') = \n",
    "\\begin{pmatrix}\n",
    "k(x_1, x_1') & \\ldots & k(x_1, x_N')\\\\\n",
    " & \\vdots  & \\\\\n",
    "k(x_N, x_1') & \\ldots & k(x_N, x_N')\n",
    "\\end{pmatrix}$$\n",
    "\n",
    ", преобразования является положительно определенным, следовательно\n",
    "\n",
    "$$\n",
    "K=U^{\\top}{\\Lambda}U \\Rightarrow k_{i,j}=(\\Lambda^{1/2}U_{:,i})^{\\top}(\\Lambda^{1/2}U_{:,j})\n",
    "$$\n",
    "\n",
    ", если ввести обозначение $\\phi(x_i) = \\Lambda^{1/2}U_{:,i}$, тогда\n",
    "\n",
    "$$\n",
    "k_{i,j} = \\phi(x_i)^{\\top}\\phi(x_j) \\Rightarrow k(x,x')=\\phi(x)^{\\top}\\phi(x')\n",
    "$$\n",
    "\n",
    "Полиномиальное ядро (polynomial kernel)\n",
    "\n",
    "$$k(x,x')=({\\gamma}x^{\\top}x' + r)^{M}, r > 0$$\n",
    "\n",
    "Расcмотрим полиномиальное ядро при $M=2, \\gamma=r=1$ и $x, x' \\in R^2$\n",
    "\n",
    "$$\n",
    "(1 + x^{\\top}{x'})^2 = (1 + {x_1}{x'_1} + {x_2}{x'_2})^2 = 1 + 2{x_1}{x'_1} + 2{x_2}{x'_2} + ({x_1}{x'_1})^2 + ({x_2}{x'_2})^2 + {x_1}{x'_1}{x_2}{x'_2}\n",
    "$$\n",
    "\n",
    "тогда \n",
    "\n",
    "$$\n",
    "\\phi(x) = [1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^{2}, x_2^{2}, \\sqrt{2}x_1x_2]^{\\top}\n",
    "$$\n",
    "\n",
    "Где мы встречались с таким преобразованием исходных признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544975d-125b-4b4c-a128-d64f539cd39b",
   "metadata": {},
   "source": [
    "## Ядерные ближайшие соседи (Kernelized Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32858b54-f37f-490d-af64-ea361851b824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c69b83b9-da6a-4538-a1b1-fb1366724e02",
   "metadata": {},
   "source": [
    "## Ядерная гребневая регрессия (Kernelized ridge regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42250af3-62d5-41a8-afb1-5e6dc31a8946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85cbab4d-2c94-46c2-9b9f-23a0303aeeab",
   "metadata": {},
   "source": [
    "## Машина разреженных векторов (Sparse vector machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57da6e2-ef49-46ee-9f80-6140a5a34ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d28458c-9763-488b-81b0-972c55f5ef8f",
   "metadata": {},
   "source": [
    "## Машина опорных векторов (Support vector  machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d262fb-22e9-4476-bc17-b0d686b3a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
    "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
    "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors=\"#FFAAAA\")\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    # plt.legend(loc=\"upper left\", fontsize=18)\n",
    "    plt.axis(axes)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "# y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + 0.1 * X**3 + np.random.randn(m, 1) / 10).ravel()\n",
    "\n",
    "epsilons = [0.1, 0.05]\n",
    "eps_names = [\"0p1\", \"0p05\"]\n",
    "for i, eps in enumerate(epsilons):\n",
    "    # svm_poly_reg1 = SVR(kernel=\"poly\", degree=5, C=1e3, epsilon=eps, gamma=\"scale\")\n",
    "    # svm_poly_reg2 = SVR(kernel=\"poly\", degree=5, C=1e-3, epsilon=eps, gamma=\"scale\")\n",
    "    svm_reg1 = SVR(kernel=\"rbf\", gamma=1, C=100, epsilon=eps)\n",
    "    svm_reg2 = SVR(kernel=\"rbf\", gamma=1, C=0.01, epsilon=eps)\n",
    "\n",
    "    svm_reg1.fit(X, y)\n",
    "    svm_reg2.fit(X, y)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
    "    plt.sca(axes[0])\n",
    "    plot_svm_regression(svm_reg1, X, y, [-1, 1, 0, 1])\n",
    "    plt.title(r\"$C={}, \\epsilon = {}$\".format(svm_reg1.C, svm_reg1.epsilon), fontsize=18)\n",
    "    plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
    "    plt.sca(axes[1])\n",
    "    plot_svm_regression(svm_reg2, X, y, [-1, 1, 0, 1])\n",
    "    plt.title(r\"$C={}, \\epsilon = {}$\".format(svm_reg2.C, svm_reg2.epsilon), fontsize=18)\n",
    "    fname = \"figures/svm_regression_e{}.pdf\".format(eps_names[i])\n",
    "    #plt.savefig(fname, dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
